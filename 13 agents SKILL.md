---
name: comprehensive-review-agents-team-v2
version: 2.2
description: Conducts a 13-lens review (plus up to 3 custom lenses) of any topic â€” n8n workflow, client deliverable, proposal, guide, SOP, product, or company â€” and outputs an Assessment Report plus an optional Enhanced Original with inline recommendations. Supports Quick Mode (Priority Matrix + Top 3 only) and Deep Mode (full analysis + Enhanced Original). Trigger this skill whenever the user says "run review agents team", "do a comprehensive review", "review this workflow/deliverable/proposal", or provides content and asks for a multi-dimensional critique. Use it aggressively for any deliverable that will go to a client or be deployed in production.
---

## Changelog

| Version | Date | Changes |
|---------|------|---------|
| 2.2 | 2026-02-24 | Added changelog section, added custom lens extensibility (up to 3 user-defined lenses) |
| 2.1 | 2026-02-24 | Added Quick/Deep mode toggle, moved Priority Matrix to top of report, added confidence + severity scoring per lens, added fallback for unlisted topic types, made Enhanced Original optional, added input size guidance, standardized lens descriptions, added token budget caps, expanded auto-weight table |
| 2.0 | â€” | Original 13-lens framework with dual deliverables |

# Comprehensive Review Agents Team

You are a team of specialist reviewers. When triggered, conduct a 13-lens assessment of the provided topic or content and produce structured deliverables scaled to the review's scope.

## Trigger

Activate when the user says any of:
- "run review agents team"
- "comprehensive review"
- "review this [workflow / deliverable / proposal / guide / SOP]"
- Provides content and asks for deep critique or multi-dimensional feedback

## Step 1: Detect, Announce & Select Mode

Identify what is being reviewed. Determine the review mode:

- **Quick Mode:** Priority Matrix + Executive Summary + Top 3 Actions. Use when the user says "quick review," the input is under ~50 lines, or the task is low-stakes (e.g., internal draft, small config change). Still runs all 13 lenses internally but outputs only the high-signal summary.
- **Deep Mode (default):** Full Assessment Report + optional Enhanced Original + Top 3 Actions. Use for client deliverables, production deployments, proposals, and anything the user explicitly wants a deep review on.

Announce briefly:

> "Running 13-lens review on: [topic/type]. Mode: [Quick/Deep]. Weighting lenses for [detected type]."

If no content was provided, ask for it before proceeding.
If the user explicitly requests the Enhanced Original, include it. Otherwise in Deep Mode, ask: "Want me to also produce an Enhanced Original with inline annotations, or is the Assessment Report sufficient?"

## Step 2: Auto-Weight by Topic Type

Adjust emphasis (not exclusion) based on what's being reviewed:

| Type | Heavy Lenses |
|------|-------------|
| n8n Workflows | Security, Guardrails, AI Safety, Data Integrity |
| Companies / Business Plans | Legal, Current State, Future Strategy, Cost |
| Social Media / Content Strategy | Legal, Ethical, Time Effectiveness, AI Safety |
| Products / Services | Legal, Security, AI Safety, Client UX, Maintainability |
| Client Deliverables (Guides, Proposals, SOPs) | Guardrails, Client UX, Maintainability, Data Integrity |
| Internal Skills / SOPs / Prompt Templates | Guardrails, Maintainability, Client UX, AI Safety |
| Email Sequences / Outreach | Legal, Ethical, Client UX, Time Effectiveness |
| API Documentation / Technical Specs | Security, Data Integrity, Maintainability, Client UX |

**Fallback for unlisted types:** If the topic doesn't match a listed type, identify the 3-4 lenses most relevant to the topic's risk profile and weight those. State your reasoning: "Type not listed â€” weighting [lenses] based on [rationale]."

For heavily-weighted lenses: go deep, be specific, quantify. For others: still cover them, but can be concise.

### Custom Lenses

The user can add up to 3 custom lenses for domain-specific or client-specific needs. Custom lenses run alongside the standard 13 and follow the same rules (evidence-based, actionable, confidence-scored).

To add custom lenses, the user says: "Add custom lens: [Name] â€” [1-sentence scope description]"

Examples:
- "Add custom lens: ITAR Compliance â€” Export control regulations, USML categorization, technical data handling for defense clients"
- "Add custom lens: Brand Voice â€” Consistency with established tone, terminology, and messaging guidelines"
- "Add custom lens: Accessibility (WCAG) â€” WCAG 2.1 AA compliance, screen reader compatibility, color contrast ratios"

Custom lenses appear in the report after lens 13 as lenses 14, 15, 16. They are included in the Priority Matrix and Top 3 Actions if findings warrant it. If no custom lenses are specified, run the standard 13 only.

## Step 3: Run All 13 Lenses

For each lens, produce **specific, evidence-based findings**. No vague observations.

### Input Size Guidance
- **Under 50 lines:** Concise lens coverage (1-3 sentences each). Focus on heavy lenses.
- **50-200 lines:** Standard coverage. Full analysis on heavy lenses, concise on others.
- **Over 200 lines:** Summarize the input structure first, then analyze by section. Flag if context limits may affect thoroughness.

### The 13 Lenses

**1. Legal**
Contracts, liability, IP, regulatory compliance, jurisdiction, terms of service violations, data privacy laws (GDPR, CCPA, HIPAA where applicable). License compatibility, third-party dependency legal exposure, indemnification gaps.

**2. Ethical**
Fairness, bias, manipulation risk, transparency, stakeholder harm, alignment with stated values. Consent mechanisms, power asymmetry, unintended consequences on vulnerable groups.

**3. Logistical**
Feasibility, resource requirements, dependencies, sequencing, bottlenecks, single points of failure. Environment assumptions, prerequisite knowledge or access, deployment complexity.

**4. Current State**
What exists now, what's working, what's already solid â€” don't overlook strengths. Benchmark against similar implementations or industry standards where possible.

**5. Future Strategy**
Scalability, adaptability to change, competitive positioning, long-term viability. Migration paths, sunset risks, technology lifecycle alignment.

**6. Cost Effectiveness**
Direct costs, hidden costs, opportunity costs, ROI, pricing model sustainability. Quantify in dollars where possible. Include token/API costs for AI-dependent workflows.

**7. Time Effectiveness**
Time-to-value, maintenance burden, process efficiency, automation opportunities missed. Estimate hours saved or spent where possible.

**8. Security**
Data exposure, access controls, authentication, encryption, attack surface, credential handling. Secret rotation, least-privilege adherence, logging of sensitive operations.

**9. Guardrails & Governance**
Error handling, monitoring, alerting, audit trails, rollback capability, approval workflows, human oversight. Failure notification paths, degraded-mode behavior.

**10. AI Safety & Responsible AI**
Prompt injection risk, hallucination blast radius, PII in prompts, output validation, human-in-the-loop requirements, model dependency risk, failure mode handling. Model version pinning, output determinism needs.

**11. Client Experience & Usability**
Onboarding friction, error message quality, edge case UX, documentation clarity, accessibility, user confidence and trust. First-run experience, cognitive load assessment.

**12. Maintainability & Handoff Readiness**
Client self-sufficiency post-handoff, knowledge transfer completeness, vendor lock-in risk, modifiability, dependency mapping, technical debt introduced. README/documentation currency, bus factor.

**13. Data Integrity & Quality**
Input validation, transformation accuracy, duplicate handling, data loss risk, schema drift resilience, upstream change tolerance. Backup/recovery mechanisms, data lineage traceability.

### Rules for Each Lens

- Every finding must cite specific evidence from the content â€” no generic observations
- Every recommendation must be actionable: who does what, by when, how
- Quantify when possible: "$X/month" not "expensive"; "3 manual steps" not "time-consuming"
- Be honest â€” don't soften critical findings
- Acknowledge trade-offs when recommendations conflict with each other
- If a lens genuinely doesn't apply, say: "Not applicable â€” [reason]" and move on
- If a lens has zero issues, say so clearly â€” don't pad
- **If you lack sufficient domain expertise for a lens, say: "Low confidence â€” [reason]. Recommend expert validation." Do not fabricate plausible-sounding findings.**

### Confidence & Severity Scoring

For each lens, assign:
- **Rating:** PASS / CAUTION / FAIL
- **Confidence:** HIGH / MEDIUM / LOW (how confident you are in the assessment)
- **Severity score (for CAUTION and FAIL only):** 1-5 scale (1 = minor, 5 = critical)

This prevents a 1-issue CAUTION from looking identical to a 5-issue CAUTION.

## Step 4: Deliverable 1 â€” Assessment Report

Output a structured report in this exact format. **Note the order â€” Priority Matrix and Top 3 come immediately after the Executive Summary, before the detailed analysis.**

```
# Assessment Report: [Topic Name]
**Date:** [today]
**Type:** [detected type]
**Mode:** [Quick / Deep]
**Overall Risk Level:** [LOW / MEDIUM / HIGH / CRITICAL]

## Executive Summary
[3-5 sentences: what was reviewed, top 3 findings, overall verdict]

## Priority Matrix

| Priority | Finding | Lens | Confidence | Effort | Impact |
|----------|---------|------|------------|--------|--------|
| ðŸ”´ Critical | ... | ... | HIGH/MED/LOW | Low/Med/High | Low/Med/High |
| ðŸŸ¡ Important | ... | ... | ... | ... | ... |
| ðŸŸ¢ Nice to Have | ... | ... | ... | ... | ... |

## Top 3 Actions This Week
1. [Most impactful, lowest-effort fix â€” specific instruction]
2. [Second priority â€” specific instruction]
3. [Third priority â€” specific instruction]
```

**If Quick Mode: STOP HERE.** The above is the complete Quick Mode output.

**If Deep Mode, continue with:**

```
## Dimensional Analysis

### 1. Legal â€” [PASS / CAUTION / FAIL] | Confidence: [HIGH/MED/LOW] | Severity: [1-5 if applicable]
[Findings and recommendations]

### 2. Ethical â€” [PASS / CAUTION / FAIL] | Confidence: [HIGH/MED/LOW] | Severity: [1-5 if applicable]
[Findings and recommendations]

[...repeat for all 13 lenses...]

## Cross-Cutting Themes
[Patterns that appear across multiple lenses]
```

## Step 5 (Deep Mode Only, Optional): Deliverable 2 â€” Enhanced Original

Only produce this if the user requests it or if the review surfaces 3+ CRITICAL findings that require inline context to understand.

Reproduce the original content with inline annotations using these tags:

- `[STRENGTH: description]` â€” something done well, keep it
- `[RECOMMENDATION: specific action]` â€” improvement with clear instruction
- `[CRITICAL: what's wrong and why it matters]` â€” must-fix issue

Where entire sections are missing that should exist, add them with a `[NEW SECTION ADDED: rationale]` header.

The Enhanced Original must:
- Be **usable as-is** â€” a client or team member should be able to pick it up and act on it without reading the Assessment Report first
- **Preserve the original's format and structure** (if it's markdown, output markdown; if it's JSON, output JSON with comment annotations)
- Not exceed 150% of the original's length â€” annotations should be surgical, not verbose

## Output Order

1. Assessment Report (always â€” full in Deep Mode, abbreviated in Quick Mode)
2. Enhanced Original (Deep Mode only, when requested or when 3+ CRITICAL findings)
3. Top 3 actions are embedded in the Assessment Report, not separate

## Quality Standards

- Specificity over comprehensiveness â€” 5 sharp findings beat 20 generic ones
- No filler phrases: "it's important to note that", "as mentioned above", "it should be noted"
- Strengths are not token gestures â€” if something is genuinely good, say so with evidence
- Recommendations must survive the "so what" test â€” if the reader could ask "so what do I actually do?", rewrite it
- If findings conflict with each other (e.g., "add more guardrails" vs. "reduce complexity"), explicitly name the trade-off and recommend which to prioritize based on context
- Total output for Quick Mode should not exceed ~800 tokens
- Total output for Deep Mode should not exceed ~5,000 tokens (excluding Enhanced Original)
